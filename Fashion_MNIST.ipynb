{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion-MNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "s47z2_a7HyHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fashion-MNIST\n",
        "\n",
        "This intro use the updated dataset fashion-mnist from Zalando.\n",
        "The dataset contains 28x28 images of clothes.\n",
        "\n",
        "We will start by setting up the environment."
      ]
    },
    {
      "metadata": {
        "id": "PHcIBpSON_LQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gzip\n",
        "\n",
        "import numpy\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "\n",
        "from tensorflow.contrib.learn.python.learn.datasets import base\n",
        "from tensorflow.python.framework import dtypes\n",
        "\n",
        "SOURCE_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n",
        "\n",
        "\n",
        "def _read32(bytestream):\n",
        "    dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
        "    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
        "\n",
        "\n",
        "def extract_images(f):\n",
        "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n",
        "\n",
        "    Args:\n",
        "    f: A file object that can be passed into a gzip reader.\n",
        "\n",
        "    Returns:\n",
        "    data: A 4D uint8 numpy array [index, y, x, depth].\n",
        "\n",
        "    Raises:\n",
        "    ValueError: If the bytestream does not start with 2051.\n",
        "\n",
        "    \"\"\"\n",
        "    print('Extracting', f.name)\n",
        "    with gzip.GzipFile(fileobj=f) as bytestream:\n",
        "        magic = _read32(bytestream)\n",
        "        if magic != 2051:\n",
        "            raise ValueError('Invalid magic number %d in MNIST image file: %s' %\n",
        "                           (magic, f.name))\n",
        "        num_images = _read32(bytestream)\n",
        "        rows = _read32(bytestream)\n",
        "        cols = _read32(bytestream)\n",
        "        buf = bytestream.read(rows * cols * num_images)\n",
        "        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
        "        data = data.reshape(num_images, rows, cols, 1)\n",
        "        return data\n",
        "\n",
        "\n",
        "def dense_to_one_hot(labels_dense, num_classes):\n",
        "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
        "    num_labels = labels_dense.shape[0]\n",
        "    index_offset = numpy.arange(num_labels) * num_classes\n",
        "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
        "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "    return labels_one_hot\n",
        "\n",
        "\n",
        "def extract_labels(f, one_hot=False, num_classes=10):\n",
        "    \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n",
        "\n",
        "    Args:\n",
        "    f: A file object that can be passed into a gzip reader.\n",
        "    one_hot: Does one hot encoding for the result.\n",
        "    num_classes: Number of classes for the one hot encoding.\n",
        "\n",
        "    Returns:\n",
        "    labels: a 1D uint8 numpy array.\n",
        "\n",
        "    Raises:\n",
        "    ValueError: If the bystream doesn't start with 2049.\n",
        "    \"\"\"\n",
        "    print('Extracting', f.name)\n",
        "    with gzip.GzipFile(fileobj=f) as bytestream:\n",
        "        magic = _read32(bytestream)\n",
        "        if magic != 2049:\n",
        "            raise ValueError('Invalid magic number %d in MNIST label file: %s' %\n",
        "                           (magic, f.name))\n",
        "        num_items = _read32(bytestream)\n",
        "        buf = bytestream.read(num_items)\n",
        "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
        "        if one_hot:\n",
        "            return dense_to_one_hot(labels, num_classes)\n",
        "        return labels\n",
        "\n",
        "\n",
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self,\n",
        "               images,\n",
        "               labels,\n",
        "               fake_data=False,\n",
        "               one_hot=False,\n",
        "               dtype=dtypes.float32,\n",
        "               reshape=True):\n",
        "        \"\"\"Construct a DataSet.\n",
        "        one_hot arg is used only if fake_data is true.  `dtype` can be either\n",
        "        `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n",
        "        `[0, 1]`.\n",
        "        \"\"\"\n",
        "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
        "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
        "            raise TypeError('Invalid image dtype %r, expected uint8 or float32' %\n",
        "                          dtype)\n",
        "        if fake_data:\n",
        "            self._num_examples = 10000\n",
        "            self.one_hot = one_hot\n",
        "        else:\n",
        "            assert images.shape[0] == labels.shape[0], ('images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n",
        "            self._num_examples = images.shape[0]\n",
        "\n",
        "            # Convert shape from [num examples, rows, columns, depth]\n",
        "            # to [num examples, rows*columns] (assuming depth == 1)\n",
        "            if reshape:\n",
        "                assert images.shape[3] == 1\n",
        "                images = images.reshape(images.shape[0],\n",
        "                                        images.shape[1] * images.shape[2])\n",
        "            if dtype == dtypes.float32:\n",
        "                # Convert from [0, 255] -> [0.0, 1.0].\n",
        "                images = images.astype(numpy.float32)\n",
        "                images = numpy.multiply(images, 1.0 / 255.0)\n",
        "        self._images = images\n",
        "        self._labels = labels\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        return self._images\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "\n",
        "    def next_batch(self, batch_size, fake_data=False):\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "        if fake_data:\n",
        "            fake_image = [1] * 784\n",
        "            if self.one_hot:\n",
        "                fake_label = [1] + [0] * 9\n",
        "            else:\n",
        "                fake_label = 0\n",
        "            return [fake_image for _ in xrange(batch_size)], [\n",
        "              fake_label for _ in xrange(batch_size)\n",
        "            ]\n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "            # Finished epoch\n",
        "            self._epochs_completed += 1\n",
        "            # Shuffle the data\n",
        "            perm = numpy.arange(self._num_examples)\n",
        "            numpy.random.shuffle(perm)\n",
        "            self._images = self._images[perm]\n",
        "            self._labels = self._labels[perm]\n",
        "            # Start next epoch\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size\n",
        "            assert batch_size <= self._num_examples\n",
        "        end = self._index_in_epoch\n",
        "        return self._images[start:end], self._labels[start:end]\n",
        "\n",
        "\n",
        "def read_data_sets(train_dir,\n",
        "                   fake_data=False,\n",
        "                   one_hot=False,\n",
        "                   dtype=dtypes.float32,\n",
        "                   reshape=True,\n",
        "                   validation_size=5000):\n",
        "    if fake_data:\n",
        "\n",
        "        def fake():\n",
        "            return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n",
        "\n",
        "        train = fake()\n",
        "        validation = fake()\n",
        "        test = fake()\n",
        "        return base.Datasets(train=train, validation=validation, test=test)\n",
        "\n",
        "    TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
        "    TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
        "    TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
        "    TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
        "\n",
        "    local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n",
        "                                   SOURCE_URL + TRAIN_IMAGES)\n",
        "    with open(local_file, 'rb') as f:\n",
        "        train_images = extract_images(f)\n",
        "\n",
        "        local_file = base.maybe_download(TRAIN_LABELS, train_dir,\n",
        "                                   SOURCE_URL + TRAIN_LABELS)\n",
        "    with open(local_file, 'rb') as f:\n",
        "        train_labels = extract_labels(f, one_hot=one_hot)\n",
        "\n",
        "        local_file = base.maybe_download(TEST_IMAGES, train_dir,\n",
        "                                   SOURCE_URL + TEST_IMAGES)\n",
        "    with open(local_file, 'rb') as f:\n",
        "        test_images = extract_images(f)\n",
        "\n",
        "        local_file = base.maybe_download(TEST_LABELS, train_dir,\n",
        "                                   SOURCE_URL + TEST_LABELS)\n",
        "    with open(local_file, 'rb') as f:\n",
        "        test_labels = extract_labels(f, one_hot=one_hot)\n",
        "\n",
        "    if not 0 <= validation_size <= len(train_images):\n",
        "        raise ValueError(\n",
        "            'Validation size should be between 0 and {}. Received: {}.'\n",
        "            .format(len(train_images), validation_size))\n",
        "\n",
        "    validation_images = train_images[:validation_size]\n",
        "    validation_labels = train_labels[:validation_size]\n",
        "    train_images = train_images[validation_size:]\n",
        "    train_labels = train_labels[validation_size:]\n",
        "\n",
        "    train = DataSet(train_images, train_labels, dtype=dtype, reshape=reshape)\n",
        "    validation = DataSet(validation_images,\n",
        "                       validation_labels,\n",
        "                       dtype=dtype,\n",
        "                       reshape=reshape)\n",
        "    test = DataSet(test_images, test_labels, dtype=dtype, reshape=reshape)\n",
        "\n",
        "    return base.Datasets(train=train, validation=validation, test=test)\n",
        "\n",
        "\n",
        "def load_mnist(train_dir='MNIST-data'):\n",
        "    return read_data_sets(train_dir)\n",
        "  \n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIUkoXEjHyHC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q import-ipynb\n",
        "\n",
        "# Import libraries\n",
        "import import_ipynb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Import Fashion MNIST dataset\n",
        "fashion_mnist = read_data_sets('input/data', one_hot=True)\n",
        "\n",
        "\n",
        "print(\"Done\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1NDiih-HyHK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Print the shape of the data\n",
        "Display the shape of data in terms of rows and columns of training and test data\n"
      ]
    },
    {
      "metadata": {
        "id": "a5wIgPQ4HyHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Shapes of training set\n",
        "print(\"Training set (images) shape: {shape}\".format(shape=fashion_mnist.train.images.shape))\n",
        "print(\"Training set (labels) shape: {shape}\".format(shape=fashion_mnist.train.labels.shape))\n",
        "print(\"-\"*60)\n",
        "# Shapes of test set\n",
        "print(\"Test set (images) shape: {shape}\".format(shape=fashion_mnist.test.images.shape))\n",
        "print(\"Test set (labels) shape: {shape}\".format(shape=fashion_mnist.test.labels.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDx9m5PQHyHQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's visualize some of the data\n",
        "\n",
        "The intro does say it is images of clothes - but let's find out ourselves. Run the cell\n",
        "\n",
        "Go ahead and choose an image index yourself. Change the \"sample_index_1/2\" variables under the red text below\"\n"
      ]
    },
    {
      "metadata": {
        "id": "4iSkj1DgHyHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dictionary of target classes\n",
        "label_dict = { 0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker',\n",
        " 8: 'Bag', 9: 'Ankle boot'}\n",
        "\n",
        "\"\"\"\n",
        "    Try changing the number in sample_index_1/2\n",
        "    Then run the cell.\n",
        "    Did the images update?\n",
        "\"\"\"\n",
        "sample_index_1 = 47\n",
        "sample_index_2 = 27\n",
        "\n",
        "fig=plt.figure(figsize=(8,8))\n",
        "# Get 28x28 image\n",
        "sample_1 = fashion_mnist.train.images[sample_index_1].reshape(28,28)\n",
        "# Get corresponding integer label from one-hot encoded data\n",
        "sample_label_1 = np.where(fashion_mnist.train.labels[sample_index_1] == 1)[0][0]\n",
        "# Plot sample\n",
        "print(\"y = {label_index} ({label})\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.imshow(sample_1, cmap='Greys')\n",
        "\n",
        "# Get 28x28 image\n",
        "sample_2 = fashion_mnist.train.images[sample_index_2].reshape(28,28)\n",
        "# Get corresponding integer label from one-hot encoded data\n",
        "sample_label_2 = np.where(fashion_mnist.train.labels[sample_index_2] == 1)[0][0]\n",
        "# Plot sample\n",
        "print(\"y = {label_index} ({label})\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.imshow(sample_2, cmap='Greys')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KIfS0BEQHyHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building our network\n",
        "\n",
        "Hopefully you're not tired of all the boring stuff like importing data.\n",
        "Let's begin with the fun part.\n",
        "\n",
        "## Steps\n",
        "#### 1 -  Set network parameters\n",
        "#### 2 - Create placeholders\n",
        "#### 3 - Initialize placeholders\n",
        "#### 4 - Forward propagation\n",
        "#### 5 - Compute cost\n",
        "#### 6 - Backpropagation\n",
        "#### 7 - Put it together\n",
        "\n",
        "## 1 - Set network parameters\n",
        "\n",
        "- The first layer: Input layer\n",
        "The first layer consist of the number of input nodes. In out case, this is 28 pixel x 28 pixel = 784.\n",
        "\n",
        "- The second and third layer: Hidden layers\n",
        "The second and third layer are hidden layers, and you can decide yourself how many nodes each hidden layer will have. Here you can test with different number of nodes.\n",
        "\n",
        "- The fourth layer: Out layer\n",
        "The output layer consists of number of classes it is able to predict. In our case, this is 10. Which node that gets chosen is determined by the softmax function.\n"
      ]
    },
    {
      "metadata": {
        "id": "3ioXTbobHyHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK 1 - Deside layers\n",
        "\n",
        "- What is the number of input nodes? (hint: pizel size of an image)\n",
        "- Explore different node sizes of the hidden layers\n",
        "- What is the output node size? (hint: number of labels"
      ]
    },
    {
      "metadata": {
        "id": "S7c5ITAnHyHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set network parameters\n",
        "n_input = XX # Fashion MNIST data input (image: 28*28=784)\n",
        "n_hidden_1 = 128 # Units in first hidden layer\n",
        "n_hidden_2 = 128 # Units in second hidden layer\n",
        "n_classes = XX # Fashion MNIST total classes (10 different target clothes)\n",
        "n_samples = fashion_mnist.train.num_examples # Number of examples in training set \n",
        "\n",
        "print(\"Number of training samples: \" + str(n_samples))\n",
        "print(\"\\nDone\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HyJI6ttLHyHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Create placeholders\n",
        "\n",
        "Creating a function which takes some info about the dimensions of the data - and returns Tensorflow placeholders. We will pass data while running our nerual network using these placeholders:"
      ]
    },
    {
      "metadata": {
        "id": "cs0C6n-vHyHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create placeholders\n",
        "def create_placeholders(n_x, n_y):\n",
        "    '''\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "\n",
        "    Arguments:\n",
        "    n_x -- scalar, size of an image vector (28*28 = 784)\n",
        "    n_y -- scalar, number of classes (10)\n",
        "\n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
        "    '''\n",
        "\n",
        "    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
        "    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
        " \n",
        "    return X, Y\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d45cZoOzHyHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Initialize Placeholders\n",
        "\n",
        "Initialize placeholders:\n",
        "Here we initialize our weights with values."
      ]
    },
    {
      "metadata": {
        "id": "Nhib9vGxHyHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters():\n",
        "    '''\n",
        "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [n_hidden_1, n_input]\n",
        "                        b1 : [n_hidden_1, 1]\n",
        "                        W2 : [n_hidden_2, n_hidden_1]\n",
        "                        b2 : [n_hidden_2, 1]\n",
        "                        W3 : [n_classes, n_hidden_2]\n",
        "                        b3 : [n_classes, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    '''\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    tf.set_random_seed(42)\n",
        "    \n",
        "    # Initialize weights and biases for each layer\n",
        "    # First hidden layer\n",
        "    W1 = tf.get_variable(\"W1\", [n_hidden_1, n_input], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
        "    b1 = tf.get_variable(\"b1\", [n_hidden_1, 1], initializer=tf.zeros_initializer())\n",
        "    \n",
        "    # Second hidden layer\n",
        "    W2 = tf.get_variable(\"W2\", [n_hidden_2, n_hidden_1], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
        "    b2 = tf.get_variable(\"b2\", [n_hidden_2, 1], initializer=tf.zeros_initializer())\n",
        "    \n",
        "    # Output layer\n",
        "    W3 = tf.get_variable(\"W3\", [n_classes, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
        "    b3 = tf.get_variable(\"b3\", [n_classes, 1], initializer=tf.zeros_initializer())\n",
        "    \n",
        "    # Store initializations as a dictionary of parameters\n",
        "    parameters = {\n",
        "        \"W1\": W1,\n",
        "        \"b1\": b1,\n",
        "        \"W2\": W2,\n",
        "        \"b2\": b2,\n",
        "        \"W3\": W3,\n",
        "        \"b3\": b3\n",
        "    }\n",
        "    \n",
        "    return parameters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpDk2oQ3HyHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Forward propagation\n",
        "\n",
        "Forward propagation is how our neural network will make a prediction given our input image. It all starts by putting an image into the input layer. From there on, the data is processed forward from input layer -> hidden layer 1 -> hidden layer 2 -> out layer -> prediction of class."
      ]
    },
    {
      "metadata": {
        "id": "xDyVheKcHyHt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK 2 - Configure the last layer on the neural network\n",
        "\n",
        "Take a look the layer Z1 and Z2 - and configure Z3."
      ]
    },
    {
      "metadata": {
        "id": "Nw9G7vYKHyHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    '''\n",
        "    Implements the forward propagation for the model: \n",
        "    LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    '''\n",
        "    \n",
        "    # Retrieve parameters from dictionary\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "    \n",
        "    # Carry out forward propagation      \n",
        "    Z1 = tf.add(tf.matmul(W1,X), b1)     \n",
        "    A1 = tf.nn.relu(Z1)                  \n",
        "    Z2 = tf.add(tf.matmul(W2,A1), b2)    \n",
        "    A2 = tf.nn.relu(Z2)                  \n",
        "    Z3 = \n",
        "    \n",
        "    return Z3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gOcGJPFOHyHy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 - Compute the cost\n",
        "\n",
        "Next, we can create a function to compute the cost based on the output from our last linear layer (Z3) and the actual target classes (Y). The cost is just a measure of the difference between the target class predicted by our neural network and the actual target class in Y.\n",
        "\n",
        "This cost will be used during backpropagation to update the weights — the greater the cost, the greater the magnitude of each parameter update. "
      ]
    },
    {
      "metadata": {
        "id": "2lo0FLXdHyHz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(Z3, Y):\n",
        "    '''\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (10, number_of_examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    '''\n",
        "    \n",
        "    # Get logits (predictions) and labels\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    # Compute cost\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQxl7KedHyH4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6 - Backpropagation\n",
        "\n",
        "Backprop is an essential step for our neural network, because it decides how much to update our parameters (weights and biases) based on the cost from the previous computation.\n",
        "\n",
        "Our implementation uses the well-known Adam optimiser for backpropagation. We will implement this in the following step where we assemble our model."
      ]
    },
    {
      "metadata": {
        "id": "5sOCBPrNHyH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7 - Putting it all together\n",
        "\n",
        "Finally, let’s combine all our previously-created functions into one single function called model(). This function will create our placeholders, initialise parameters, carry out forward prop and backprop, and also help us visualise our cost decrease over time.\n",
        "\n",
        "The function will take in the training and the test sets, an option to print out costs after each epoch, as well as some (optional) hyperparameter values such as:\n",
        "\n",
        "- learning_rate (learning rate of the optimisation)\n",
        "- num_epochs (number of epochs of the optimisation loop)\n",
        "- minibatch_size (size of a minibatch)\n",
        "\n",
        "After building the network and training it, the function will compute the model’s accuracies, and return the final updated parameters dictionary."
      ]
    },
    {
      "metadata": {
        "id": "W-FxUVhFHyH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(train, test, learning_rate=0.0001, num_epochs=16, minibatch_size=32, print_cost=True, graph_filename='costs'):\n",
        "    '''\n",
        "    Implements a four-layer tensorflow neural network: INPUT->HIDDEN1->HIDDEN2->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    train -- training set\n",
        "    test -- test set\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every epoch\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    '''\n",
        "    \n",
        "    print(\"Setting up model..\")\n",
        "\n",
        "    \n",
        "    # Ensure that model can be rerun without overwriting tf variables\n",
        "    ops.reset_default_graph()\n",
        "    # For reproducibility\n",
        "    tf.set_random_seed(42)\n",
        "    seed = 42\n",
        "    # Get input and output shapes\n",
        "    (n_x, m) = train.images.T.shape\n",
        "    n_y = train.labels.T.shape[0]\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    # Create placeholders of shape (n_x, n_y)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters()\n",
        "    \n",
        "    # Forward propagation\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    # Compute cost\n",
        "    cost = compute_cost(Z3, Y)\n",
        "    # Backpropagation (using Adam optimizer)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "    \n",
        "    # Initialize variables\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    true_label_last_mini_batch = []\n",
        "    \n",
        "    # Start session to compute Tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Initializing Tensorflow session..\")\n",
        "        # Run initialization\n",
        "        sess.run(init)\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            epoch_cost = 0.\n",
        "            num_minibatches = int(m / minibatch_size)\n",
        "            seed = seed + 1\n",
        "            \n",
        "            for i in range(num_minibatches):\n",
        "                \n",
        "                # Get next batch of training data and labels\n",
        "                minibatch_X, minibatch_Y = train.next_batch(minibatch_size)\n",
        "                \n",
        "                # Execute optimizer and cost function\n",
        "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X.T, Y: minibatch_Y.T})\n",
        "                      \n",
        "                # Update epoch cost\n",
        "                epoch_cost += minibatch_cost / num_minibatches\n",
        "                \n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True:\n",
        "                print(\"Cost after epoch {epoch_num}: {cost}\".format(epoch_num=epoch, cost=epoch_cost))\n",
        "                costs.append(epoch_cost)\n",
        "        \n",
        "        # Plot costs\n",
        "        #plt.figure(figsize=(16,5))\n",
        "        #plt.plot(np.squeeze(costs), color='#2A688B')\n",
        "        plt.xlim(0, num_epochs-1)\n",
        "        plt.ylabel(\"cost\")\n",
        "        plt.xlabel(\"iterations\")\n",
        "        plt.title(\"learning rate = {rate}\".format(rate=learning_rate))\n",
        "        plt.savefig(graph_filename, dpi=300)\n",
        "        plt.show()\n",
        "        \n",
        "        # Save parameters\n",
        "        parameters = sess.run(parameters)\n",
        "        print(\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "        \n",
        "        # Calculate accuracy on test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "        \n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: train.images.T, Y: train.labels.T}))\n",
        "        print (\"Test Accuracy:\", accuracy.eval({X: test.images.T, Y: test.labels.T}))\n",
        "        \n",
        "        \n",
        "        # Plot a random sample of 10 test images, their predicted labels and ground truth\n",
        "        figure = plt.figure(figsize=(20, 8))\n",
        "        for i, index in enumerate(np.random.choice(test.images.shape[0], size=15, replace=False)):\n",
        "            ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "            # Display each image\n",
        "            ax.imshow(np.squeeze(test.images[index].reshape(28,28)),cmap='Greys')\n",
        "            predict_index = np.argmax(Z3.eval(feed_dict={X:test.images[index].reshape(784,1)}))\n",
        "            true_index = np.argmax(test.labels[index])\n",
        "            # Set the title for each image\n",
        "            ax.set_title(\"{} (solution: {})\".format(label_dict[predict_index], \n",
        "                                          label_dict[true_index]),\n",
        "                                          color=(\"green\" if predict_index == true_index else \"red\"))\n",
        "\n",
        "        return parameters\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ApEalal8HyIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TASK 3 - Run with different input parameters \n",
        "\n",
        "Experiment and decide the following parameters:\n",
        "\n",
        "- learning_rate\n",
        "- num_epochs\n",
        "- minibatch_size\n",
        "\n",
        "Let's do this"
      ]
    },
    {
      "metadata": {
        "id": "R18W1YsMHyIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Running our model\n",
        "train = fashion_mnist.train\n",
        "test = fashion_mnist.test\n",
        "\n",
        "# Decide \n",
        "learning_rate = # Choose between 0.0001 and 0.1\n",
        "num_epochs = # Choose between 1 and 100. More epochs = more training\n",
        "minibatch_size = # Choose number of images each bach should coontain. Between 1 and 55000\n",
        "\n",
        "parameters = model(train, test, learning_rate, num_epochs, minibatch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
